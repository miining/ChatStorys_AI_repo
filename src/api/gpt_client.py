from typing import Dict, List, Optional
from openai import OpenAI
import os
from dotenv import load_dotenv
from ..utils.prompt_templates import PromptTemplates

load_dotenv()

class GPTClient:
    def __init__(self, api_key: str = None, model: str = None):
        """
        Initialize GPT client for novel generation with enhanced chat history support
        
        Args:
            api_key: OpenAI API key
            model: Model name to use (default from env or gpt-4)
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OpenAI API key is required")
        
        # Initialize OpenAI client with new API
        self.client = OpenAI(api_key=self.api_key)
        
        # Model configuration from environment or default
        self.model = model or os.getenv("OPENAI_MODEL", "gpt-4")
        self.max_tokens = int(os.getenv("MAX_TOKENS", "1500"))
        self.temperature = float(os.getenv("TEMPERATURE", "0.7"))
        
        # Chat history management settings
        self.max_history_tokens = int(os.getenv("MAX_HISTORY_TOKENS", "8000"))  # íˆìŠ¤í† ë¦¬ í† í° í•œë„
        self.max_context_length = int(os.getenv("MAX_CONTEXT_LENGTH", "16000"))  # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ í•œë„

    def _estimate_tokens(self, text: str) -> int:
        """
        Estimate token count for text (rough estimation: 1 token â‰ˆ 3.5 characters for Korean/English mix)
        """
        return max(1, len(text) // 3)

    def _prepare_chat_history(self, messages: List[Dict], current_user_message: str, system_prompt: str) -> List[Dict]:
        """
        Prepare chat history while staying within token limits
        ìµœëŒ€í•œ ë§ì€ íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ë©´ì„œ í† í° í•œë„ë¥¼ ì¤€ìˆ˜
        
        Args:
            messages: ì±„íŒ… íˆìŠ¤í† ë¦¬
            current_user_message: í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            
        Returns:
            í† í° í•œë„ ë‚´ì˜ ìµœì í™”ëœ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        """
        if not messages:
            return []
        
        # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
        system_tokens = self._estimate_tokens(system_prompt)
        current_tokens = self._estimate_tokens(current_user_message)
        available_tokens = self.max_context_length - self.max_tokens - system_tokens - current_tokens - 500  # ì—¬ìœ ë¶„
        
        chat_history = []
        total_tokens = 0
        
        # ìµœì‹  ë©”ì‹œì§€ë¶€í„° ì—­ìˆœìœ¼ë¡œ ì¶”ê°€ (ìµœì‹  ì»¨í…ìŠ¤íŠ¸ ìš°ì„  ë³´ì¡´)
        for msg in reversed(messages):
            msg_tokens = 0
            temp_messages = []
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ì™€ AI ì‘ë‹µì„ ì„¸íŠ¸ë¡œ ì²˜ë¦¬
            if "User" in msg and "LLM_Model" in msg:
                user_content = msg["User"]
                assistant_content = msg["LLM_Model"]
                
                user_tokens = self._estimate_tokens(user_content)
                assistant_tokens = self._estimate_tokens(assistant_content)
                msg_tokens = user_tokens + assistant_tokens
                
                if total_tokens + msg_tokens <= available_tokens:
                    temp_messages = [
                        {"role": "user", "content": user_content},
                        {"role": "assistant", "content": assistant_content}
                    ]
                else:
                    # í† í° í•œë„ ì´ˆê³¼ì‹œ ì¤‘ë‹¨
                    break
                    
            elif "User" in msg:
                user_content = msg["User"]
                msg_tokens = self._estimate_tokens(user_content)
                
                if total_tokens + msg_tokens <= available_tokens:
                    temp_messages = [{"role": "user", "content": user_content}]
                else:
                    break
                    
            elif "LLM_Model" in msg:
                assistant_content = msg["LLM_Model"]
                msg_tokens = self._estimate_tokens(assistant_content)
                
                if total_tokens + msg_tokens <= available_tokens:
                    temp_messages = [{"role": "assistant", "content": assistant_content}]
                else:
                    break
            
            # ë©”ì‹œì§€ ì¶”ê°€ (ì•ìª½ì— ì‚½ì…í•˜ì—¬ ì‹œê°„ìˆœ ìœ ì§€)
            for temp_msg in reversed(temp_messages):
                chat_history.insert(0, temp_msg)
            total_tokens += msg_tokens
        
        return chat_history

    def chat_session(self, chapter_num: str, context: Dict, user_message: str, messages: List[Dict] = None) -> str:
        """
        Generate novel content using comprehensive chat history and optimized prompts
        
        Args:
            chapter_num: í˜„ì¬ ì±•í„° ë²ˆí˜¸
            context: ì†Œì„¤ ì»¨í…ìŠ¤íŠ¸ (ì´ì „ ì±•í„°, ì±… ì •ë³´ ë“±)
            user_message: ì‚¬ìš©ì ì…ë ¥
            messages: ì±„íŒ… íˆìŠ¤í† ë¦¬ (ìµœëŒ€í•œ ë§ì´ í™œìš©)
            
        Returns:
            ìƒì„±ëœ ì†Œì„¤ ë‚´ìš©
        """
        try:
            # PromptTemplatesì—ì„œ ì „ë¬¸ì ì¸ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            system_prompt = PromptTemplates.get_chapter_prompt(chapter_num, context)
            
            # ë©”ì‹œì§€ êµ¬ì„± ì‹œì‘
            chat_messages = [{"role": "system", "content": system_prompt}]
            
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ ìµœëŒ€í•œ ë§ì´ ì¶”ê°€ (í† í° í•œë„ ë‚´ì—ì„œ)
            if messages:
                history_messages = self._prepare_chat_history(messages, user_message, system_prompt)
                chat_messages.extend(history_messages)
                print(f"ğŸ“š Chat history: {len(history_messages)} messages loaded")
            
            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            chat_messages.append({"role": "user", "content": user_message})
            
            # ì†Œì„¤ ìƒì„±ì— ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ìš”ì²­
            response = self.client.chat.completions.create(
                model=self.model,
                messages=chat_messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                presence_penalty=0.3,   # ìƒˆë¡œìš´ ì•„ì´ë””ì–´ ìƒì„± ì¥ë ¤
                frequency_penalty=0.2,  # ë°˜ë³µ ê°ì†Œ
                top_p=0.95,            # ì°½ì˜ì„±ì„ ìœ„í•œ nucleus sampling
                stream=False
            )
            
            generated_content = response.choices[0].message.content
            print(f"âœ… Generated {len(generated_content)} characters")
            return generated_content
            
        except Exception as e:
            raise Exception(f"Error generating novel content: {str(e)}")

    def summarize_chapter(self, content: str, chapter_num: str = None) -> str:
        """
        Generate comprehensive chapter summary using enhanced prompt template
        
        Args:
            content: ì±•í„° ë‚´ìš©
            chapter_num: ì±•í„° ë²ˆí˜¸ (ì„ íƒì‚¬í•­)
            
        Returns:
            ì „ë¬¸ì ì¸ ì±•í„° ìš”ì•½
        """
        try:
            # PromptTemplatesì—ì„œ ì „ë¬¸ì ì¸ ìš”ì•½ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            system_prompt = PromptTemplates.get_summary_prompt(content)
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt}
                ],
                temperature=0.3,  # ìš”ì•½ì€ ì¼ê´€ì„±ì´ ì¤‘ìš”
                max_tokens=400,   # ë” ìƒì„¸í•œ ìš”ì•½ì„ ìœ„í•´ ì¦ê°€
                top_p=0.8
            )
            
            summary = response.choices[0].message.content
            print(f"ğŸ“ Chapter summary generated: {len(summary)} characters")
            return summary
            
        except Exception as e:
            raise Exception(f"Error summarizing chapter: {str(e)}")

    def generate_with_genre_requirements(self, genre: str, requirements: Dict, user_message: str) -> str:
        """
        Generate content with specific genre requirements using enhanced prompts
        
        Args:
            genre: ì†Œì„¤ ì¥ë¥´
            requirements: ì¥ë¥´ë³„ ìš”êµ¬ì‚¬í•­
            user_message: ì‚¬ìš©ì ìš”ì²­
            
        Returns:
            ì¥ë¥´ì— ë§ëŠ” ìƒì„±ëœ ë‚´ìš©
        """
        try:
            # ì¥ë¥´ë³„ ì „ë¬¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            system_prompt = PromptTemplates.get_genre_prompt(genre, requirements)
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                presence_penalty=0.2,
                frequency_penalty=0.1
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            raise Exception(f"Error generating genre-specific content: {str(e)}")

    def format_response(self, response: str, metadata: Dict = None) -> Dict:
        """
        Format response with comprehensive metadata
        
        Args:
            response: ìƒì„±ëœ ì‘ë‹µ
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            
        Returns:
            í¬ë§·ëœ ì‘ë‹µ ë”•ì…”ë„ˆë¦¬
        """
        result = {
            "content": response,
            "status": "success",
            "metadata": {
                "word_count": len(response.split()),
                "character_count": len(response),
                "estimated_tokens": self._estimate_tokens(response),
                "estimated_reading_time": max(1, len(response.split()) // 200),  # ë¶„ ë‹¨ìœ„
                "line_count": len(response.split('\n'))
            }
        }
        
        if metadata:
            result["metadata"].update(metadata)
            
        return result

    def get_model_info(self) -> Dict:
        """
        Get current model configuration information
        
        Returns:
            ëª¨ë¸ ì„¤ì • ì •ë³´
        """
        return {
            "model": self.model,
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "max_history_tokens": self.max_history_tokens,
            "max_context_length": self.max_context_length
        } 